{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a22f1cd7",
   "metadata": {},
   "source": [
    "---\n",
    "description: \"Here\\u2019s a **comprehensive, forward-looking overview of DuckDB**\\\n",
    "  \\ \\u2014 a database that\\u2019s been rapidly gaining traction for analytics workloads,\\\n",
    "  \\ especially for data engineers, ML practitioners, and Python/R developers like\\\n",
    "  \\ yourself.\"\n",
    "output-file: duckdb.html\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "title: DuckDB\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b1e95",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29545a5-7b1c-4064-90fa-ebec80a7607b",
   "metadata": {},
   "source": [
    "## ðŸ”¹ What is DuckDB?\n",
    "\n",
    "* **In-process OLAP database**: It runs inside your application (like SQLite) but is optimized for **analytics queries** (OLAP), not transactional workloads (OLTP).\n",
    "* **Zero-config**: No server to spin up, no dependencies. You just `import duckdb` and start querying.\n",
    "* **Columnar engine**: Uses a column-store format with vectorized execution, making it blazing fast for aggregation and scanning large datasets.\n",
    "* **Modern SQL dialect**: Full SQL support (joins, windows, CTEs, JSON, UDFs).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85f8be1-c829-4c3e-a112-8904ad3ae25a",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Key Features\n",
    "\n",
    "### 1. **Data Access**\n",
    "\n",
    "* Reads CSV, Parquet, JSON, SQLite, Pandas, Polars, Arrow, Iceberg, MotherDuck (cloud).\n",
    "* Can **query files directly** (e.g., `SELECT * FROM 'file.parquet'`).\n",
    "* Can query **in-memory dataframes** without copying.\n",
    "\n",
    "### 2. **Performance**\n",
    "\n",
    "* **Vectorized execution**: Processes chunks of columns in CPU cache.\n",
    "* **Parallelism**: Multithreaded query execution.\n",
    "* **Efficient compression**: Similar to Parquet, built for scan speed.\n",
    "* Benchmarks show it often beats Pandas/Polars for SQL-style analytics on millions-to-billions of rows.\n",
    "\n",
    "### 3. **Integration**\n",
    "\n",
    "* **Python / R / C++ / CLI** bindings.\n",
    "* Native support for **Pandas/Polars â†” DuckDB** interchange.\n",
    "* Can be used as a lightweight ETL or embedded query layer inside ML pipelines.\n",
    "* **dbt + DuckDB** is popular for lightweight data warehousing.\n",
    "\n",
    "### 4. **SQL Features**\n",
    "\n",
    "* Joins, aggregations, window functions, subqueries.\n",
    "* JSON functions (`json_extract`, `json_group_array`).\n",
    "* User-defined functions in Python.\n",
    "* Time-travel style queries with Iceberg.\n",
    "\n",
    "### 5. **Persistence**\n",
    "\n",
    "* Default is **in-memory**, but you can `duckdb.connect(\"file.duckdb\")` to persist.\n",
    "* Tables can be **native DuckDB storage** or **external (Parquet, CSV, Arrow)**.\n",
    "* **No server process**: library embedded in your app.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94472fc1-a174-4900-be95-62ee3ced2553",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Typical Use Cases\n",
    "\n",
    "| Use Case                      | Why DuckDB Works Well                                |\n",
    "| ----------------------------- | ---------------------------------------------------- |\n",
    "| **Ad-hoc analytics**          | Query Parquet/CSV files directly with SQL.           |\n",
    "| **Replacement for Pandas**    | Faster joins, group-bys, aggregations.               |\n",
    "| **Data science prep**         | Integrates with ML workflows (Pandas/Polars/Arrow).  |\n",
    "| **Local OLAP engine**         | Can handle 100Mâ€“1B rows in memory on a laptop.       |\n",
    "| **Embedded analytics**        | Works inside apps, no external server needed.        |\n",
    "| **ETL lightweight warehouse** | Works with dbt, Iceberg, and MotherDuck for scaling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f78a0b3-d39c-4f3b-bfcc-c8eadf3feedf",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Limitations\n",
    "\n",
    "* **Not an OLTP database**: No concurrent writes or high transaction rates.\n",
    "* **Memory-bound**: Runs in your process memory; not built for multi-terabyte datasets (unless combined with external storage).\n",
    "* **Ecosystem still maturing**: Great for analytics, but lacks advanced role/security features of Postgres.\n",
    "* **No indexes (yet)**: Optimized for full scans, not point lookups.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3cff2e-09e1-4385-b2cf-23e629ec8540",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Ecosystem\n",
    "\n",
    "* **MotherDuck**: Cloud service extending DuckDB (shared storage, scaling, persistence).\n",
    "* **dbt-duckdb**: Run dbt models against local files and Parquet with DuckDB.\n",
    "* **Polars + DuckDB**: Polars for Pythonic transformations, DuckDB for SQL joins/aggregations.\n",
    "* **Data Fusion**: Works well as a query layer over Iceberg/Parquet lakes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8464a5-cf8a-48cd-be44-ac59d41d50be",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Example Workflows\n",
    "\n",
    "### Query a Parquet file directly\n",
    "\n",
    "```python\n",
    "import duckdb\n",
    "con = duckdb.connect()\n",
    "df = con.execute(\"SELECT station, AVG(temp) FROM 'measurements.parquet' GROUP BY station\").df()\n",
    "```\n",
    "\n",
    "### Interop with Pandas\n",
    "\n",
    "```python\n",
    "import pandas as pd, duckdb\n",
    "df = pd.read_csv(\"big.csv\")\n",
    "duckdb.query(\"SELECT col1, COUNT(*) FROM df GROUP BY col1\").df()\n",
    "```\n",
    "\n",
    "### Save to persistent DB\n",
    "\n",
    "```python\n",
    "con = duckdb.connect(\"analytics.duckdb\")\n",
    "con.execute(\"CREATE TABLE IF NOT EXISTS sales AS SELECT * FROM 'sales.parquet'\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7fec3c-7d37-4bd4-854b-8bb87d71a9c2",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Performance Outlook\n",
    "\n",
    "* Handles **billions of rows** on a laptop with enough memory.\n",
    "* Benchmarks: scanning a **1B row CSV** can take \\~30â€“60s; Parquet is much faster (\\~10â€“20s).\n",
    "* With Polars/DuckDB hybrid workflows, you can approach **Rust-level performance** with the flexibility of Python.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c079f-0df7-4960-a75b-610ae345038d",
   "metadata": {},
   "source": [
    "## ðŸ”¹ When to Use vs Alternatives\n",
    "\n",
    "| Tool                            | Best For                                            |\n",
    "| ------------------------------- | --------------------------------------------------- |\n",
    "| **DuckDB**                      | Local analytics, SQL over files, Pandas replacement |\n",
    "| **SQLite**                      | Embedded OLTP, small apps                           |\n",
    "| **Polars**                      | Pythonic fast dataframe ops (no SQL)                |\n",
    "| **Pandas**                      | Legacy ecosystem, flexibility, but slower           |\n",
    "| **Postgres/BigQuery/Snowflake** | Multi-user, transactional, massive data warehouse   |\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In short**: DuckDB is the *SQLite of analytics*. Itâ€™s ideal when you want SQL-powered analytics without a big warehouse or cluster, yet still want to handle 100Mâ€“1B rows efficiently. Itâ€™s becoming a cornerstone of the **modern data stack**, especially when paired with Parquet, Arrow, and dbt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c09e48-bc6d-4bbc-bce2-b4bcea4ca555",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
